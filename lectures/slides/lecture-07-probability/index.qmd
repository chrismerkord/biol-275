---
title: |
  | Lecture 7
  | Probability
subtitle: "ABD 3e Chapter 5"
format:
  revealjs:
    fig-width: 8
    fig-height: 6
---

## Learning Objectives

By the end of this lecture, you should be able to:

-   Explain probability as long-run relative frequency and how it links samples to populations.

-   Distinguish between discrete probability distributions and continuous probability densities.

-   Apply the addition rule and multiplication rule of probability.

-   Determine whether events are mutually exclusive, independent, or dependent.

-   Compute and interpret conditional probabilities, including simple multi-step processes.

-   Use probability trees to calculate probabilities across sequential events.

## Probability theory: the foundation of statistical thinking

::::: columns
::: {.column width="60%"}
-   **Probability** is the *true relative frequency* of an event\
-   It is the proportion of times the event would occur if the same process were repeated many times\
-   Probability values range from **0 to 1**\
-   Written as **Pr(event)**
:::

::: {.column width="40%"}
![Conceptual probability scale from impossible to certain, showing that probability values range from 0 to 1, with examples including a 1-in-6 chance, an even chance, and a 4-in-5 chance. Source: Illustration generated by ChatGPT.](images/probability-scale.png){#fig-probability-scale fig-alt="A horizontal probability scale labeled from 0 (impossible) to 1 (certain), with intermediate labels unlikely, even chance, and likely, and example icons illustrating a 1-in-6 chance, a fair coin toss, and a 4-in-5 chance."}
:::
:::::

## Probability theory and statistics

-   Probability theory links **sample estimates** to **population parameters**
-   Samples:
    -   Come from data
    -   Vary by chance
-   Populations:
    -   Usually unobserved
    -   Have fixed values
-   Probability explains how chance affects what we observe

## [Venn diagrams]{.keyword} and sample space

::::: columns
::: {.column width="50%"}
-   A **Venn diagram** represents all possible outcomes of a random trial
-   The **entire diagram** corresponds to the *sample space*
-   **Events** are shown as regions within the sample space
-   Larger areas represent events with **higher probability**
:::

::: {.column width="50%"}
Example: $\operatorname{Pr}[A] > \operatorname{Pr}[B]$

```{r}
#| label: fig-venn-sample-space
#| fig-cap: "Venn diagram representing the sample space of a random trial, with two mutually exclusive events where event A has a larger area than event B."
#| fig-alt: "A Venn diagram with two non-overlapping circles labeled A and B. Circle A is larger than circle B, indicating a higher probability for event A than event B. The circles are different colors and partially transparent on a transparent background."

library(ggplot2)

# Okabe-Ito colors
col_A <- "#0072B2"  # blue
col_B <- "#D55E00"  # vermillion

theta <- seq(0, 2*pi, length.out = 400)

# Event A (larger)
circle_A <- data.frame(
  x = cos(theta) * 1.25,
  y = sin(theta) * 1.25
)

# Event B (smaller, non-overlapping)
circle_B <- data.frame(
  x = cos(theta) * 0.75 + 2.7,
  y = sin(theta) * 0.75
)

ggplot() +
  geom_polygon(
    data = circle_A, aes(x, y),
    fill = col_A, color = col_A, alpha = 0.6, linewidth = 1
  ) +
  geom_polygon(
    data = circle_B, aes(x, y),
    fill = col_B, color = col_B, alpha = 0.6, linewidth = 1
  ) +
  annotate("text", x = 0,   y = 0, label = "A", size = 14, color = col_A) +
  annotate("text", x = 2.7, y = 0, label = "B", size = 14, color = col_B) +
  coord_fixed() +
  theme_void()


```
:::
:::::

## Mutually exclusive vs. non-exclusive events

::::: columns
::: {.column width="50%"}
### [Mutually exclusive]{.keyword} events

-   Cannot occur at the same time

-   Outcomes belong to only one event

$$
Pr[A \text{ and } B] = 0
$$

```{r}
#| label: fig-venn-mutually-exclusive
#| fig-height: 3.5
#| fig-cap: "Venn diagram of two mutually exclusive events, where events A and B do not overlap."
#| fig-alt: "A Venn diagram with two non-overlapping circles labeled A and B, indicating mutually exclusive events with no shared outcomes."

library(ggplot2)

# Okabe–Ito colors
col_A <- "#0072B2"
col_B <- "#D55E00"

theta <- seq(0, 2*pi, length.out = 400)

circle_A <- data.frame(
  x = cos(theta),
  y = sin(theta)
)

circle_B <- data.frame(
  x = cos(theta) + 2.4,
  y = sin(theta)
)

ggplot() +
  geom_polygon(aes(x, y), circle_A, 
               fill = col_A, color = col_A, alpha = 0.6, linewidth = 1) +
  geom_polygon(aes(x, y), circle_B, 
               fill = col_B, color = col_B, alpha = 0.6, linewidth = 1) +
  annotate("text", x = 0,   y = 0, label = "A", size = 9, color = col_A) +
  annotate("text", x = 2.4, y = 0, label = "B", size = 9, color = col_B) +
  coord_fixed(xlim = c(-1.5, 3.9), ylim = c(-1.1, 1.1)) +
  theme_void()


```
:::

::: {.column width="50%"}
### [Not mutually exclusive]{.keyword} events

-   Can occur at the same time

-   Outcomes may belong to both events

$$
Pr[A \text{ and } B] > 0
$$

```{r}
#| label: fig-venn-not-mutually-exclusive
#| fig-height: 3.5
#| fig-cap: "Venn diagram of two non-exclusive events, where events A and B overlap."
#| fig-alt: "A Venn diagram with two overlapping circles labeled A and B, showing a shared region that represents outcomes common to both events."

library(ggplot2)

# Okabe–Ito colors
col_A <- "#0072B2"
col_B <- "#D55E00"

theta <- seq(0, 2*pi, length.out = 400)

# Center the overlapping circles in the figure
circle_A <- data.frame(
  x = cos(theta) - 0.6,
  y = sin(theta)
)

circle_B <- data.frame(
  x = cos(theta) + 0.6,
  y = sin(theta)
)

ggplot() +
  geom_polygon(aes(x, y), data = circle_A,
               fill = col_A, color = col_A, alpha = 0.6, linewidth = 1) +
  geom_polygon(aes(x, y), data = circle_B,
               fill = col_B, color = col_B, alpha = 0.6, linewidth = 1) +
  annotate("text", x = -0.9, y = 0, label = "A", size = 9, color = col_A) +
  annotate("text", x =  0.9, y = 0, label = "B", size = 9, color = col_B) +
  coord_fixed(xlim = c(-2.6, 2.6), ylim = c(-1.1, 1.1)) +
  theme_void()


```
:::
:::::

## [Probability distributions]{.keyword}

::::: columns
::: {.column width="50%"}
-   The true relative frequency of all possible values of a random variable
-   Some probability distributions:
    -   Can be described mathematically
    -   Are simply a list of possible outcomes with their probabilities
:::

::: {.column width="50%"}
Example: the outcomes from rolling a fair six-sided die

![Probability distribution for the outcomes of a fair six-sided die, where each possible outcome has equal probability.](images/probability-distribution-dice.jpg){#fig-probability-distribution-dice fig-alt="A probability distribution showing six discrete outcomes from a fair die, with each outcome assigned the same probability value."}
:::
:::::

## Probability: distributions vs. densities

::::: columns
::: {.column width="50%"}
[Probability distributions]{.keyword} describe *discrete* variables

-   All discrete outcomes have **finite probabilities**
-   Probabilities across all outcomes **sum to 1**

[Probability densities]{.keyword} describe *continuous* variables

-   The probability of any exact value is **infinitesimal**
-   Probabilities are defined over **ranges**, not single values
-   The total area under the curve **integrates to 1**
:::

::: {.column width="50%"}
```{r}
#| label: fig-distribution-vs-density
#| fig-height: 7
#| fig-cap: "Comparison of a probability distribution and a probability density based on the same normal distribution. The top panel shows a discretized probability distribution, while the bottom panel shows the corresponding continuous probability density."
#| fig-alt: "Two vertically stacked plots based on a normal distribution. The top plot shows a discrete probability distribution from binning a normal distribution into intervals, with bars labeled by the bin ranges. The bottom plot shows a smooth normal probability density curve over the same range."

library(ggplot2)
library(patchwork)

# Common normal distribution parameters
mu <- 0
sigma <- 1

# Discretized probability distribution (binned normal)
breaks <- seq(-3, 3, by = 1)
left  <- head(breaks, -1)
right <- tail(breaks, -1)

dist_df <- data.frame(
  center = (left + right) / 2,
  prob   = pnorm(right, mu, sigma) - pnorm(left, mu, sigma),
  bin_label = paste0("[", left, ", ", right, ")")
)

# Ensure bins are ordered left-to-right on the x-axis
dist_df$bin_label <- factor(dist_df$bin_label, levels = dist_df$bin_label[order(dist_df$center)])

p_dist <- ggplot(dist_df, aes(x = bin_label, y = prob)) +
  geom_col(fill = "#0072B2", width = 0.9) +
  labs(
    title = "Probability distribution (binned normal)",
    x = "Value range",
    y = "Probability"
  ) +
  theme_minimal()

# Continuous probability density (normal)
x <- seq(-3.5, 3.5, length.out = 400)
dens_df <- data.frame(
  x = x,
  y = dnorm(x, mu, sigma)
)

p_dens <- ggplot(dens_df, aes(x = x, y = y)) +
  geom_area(fill = "#D55E00", alpha = 0.4) +
  geom_line(color = "#D55E00", linewidth = 1) +
  labs(
    title = "Probability density (area under the curve = 1)",
    x = "Value",
    y = "Density"
  ) +
  theme_minimal()

# Stack vertically for a roughly square figure
p_dist / p_dens

```
:::
:::::

## [Proportions]{.keyword}

-   A [proportion]{.keyword} is the number of times an event occurs divided by the number of trials
-   Range from **0 to 1**
-   A proportion can be viewed as a **realized sample** from a probability distribution
-   With more trials, proportions tend to get closer to the underlying probability

## The [General Addition Principle]{.keyword}

-   The probability of **A or B** includes all outcomes in **A**, all outcomes in **B**, and avoids double-counting outcomes in both
-   When two events overlap, their shared outcomes must be subtracted once

$$
\operatorname{Pr}[A \text{ or } B]
=
\operatorname{Pr}[A]
+
\operatorname{Pr}[B]
-
\operatorname{Pr}[A \text{ and } B]
$$

![Visual illustration of the General Addition Principle, showing that the probability of A or B equals the probability of A plus the probability of B minus the probability of their overlap.](images/addition-principle-venn.jpg){#fig-addition-principle-venn fig-alt="A Venn diagram with two overlapping circles labeled A and B. The diagram shows that the combined area representing A or B equals the area of A plus the area of B, with the overlapping region subtracted once to avoid double counting." width="1299"}

## A special case of the addition principle

::::: columns
::: {.column width="50%"}
-   When two events are **mutually exclusive**, they cannot occur at the same time
-   There is **no overlap** between events

$$
\operatorname{Pr}[A \text{ or } B]
=
\operatorname{Pr}[A]
+
\operatorname{Pr}[B]
$$
:::

::: {.column width="50%"}
![ABO and Rh blood types arranged into mutually exclusive categories, illustrating a case where events have no overlapping outcomes.](images/addition-principle-blood-types.jpg){#fig-addition-principle-blood-types fig-alt="A diagram of ABO and Rh blood types organized into non-overlapping regions, showing that each blood type category is mutually exclusive with the others."}
:::
:::::

## Example: probability of a range

::::: columns
::: {.column width="50%"}
-   Consider the **sum of two fair six-sided dice**
-   Possible sums range from **2 to 12**
-   We want the probability that the sum is **between 6 and 8**, inclusive

$$
\operatorname{Pr}[6 \text{ or } 7 \text{ or } 8]
=\\ 
\operatorname{Pr}[6] + \operatorname{Pr}[7] 
+ \operatorname{Pr}[8]
$$
:::

::: {.column width="50%"}
```{r}
#| label: fig-dice-sum-range
#| fig-cap: "Probability distribution of the sum of two fair six-sided dice, with outcomes 6 through 8 highlighted to illustrate calculating the probability of a range."
#| fig-alt: "A bar chart showing the probability distribution for the sum of two dice from 2 to 12. Bars corresponding to sums of 6, 7, and 8 are highlighted, while other sums are shown in a lighter color."

library(ggplot2)

# All possible sums of two dice
sums <- expand.grid(d1 = 1:6, d2 = 1:6)
sums$S <- sums$d1 + sums$d2

dist_df <- as.data.frame(table(sums$S))
names(dist_df) <- c("sum", "count")
dist_df$sum <- as.numeric(as.character(dist_df$sum))
dist_df$prob <- dist_df$count / sum(dist_df$count)

# Highlight range 6–8
dist_df$highlight <- dist_df$sum %in% 6:8

ggplot(dist_df, aes(x = sum, y = prob)) +
  geom_col(aes(fill = highlight), color = "grey30") +
  scale_fill_manual(
    values = c("FALSE" = "grey85", "TRUE" = "#56B4E9"),
    guide = "none"
  ) +
  labs(
    x = "Sum of two dice",
    y = "Probability"
  ) +
  scale_x_continuous(breaks = 2:12) +
  theme_minimal()

```
:::
:::::

## Example: probabilities sum to 1

::::: columns
::: {.column width="50%"}
-   Consider a **single fair six-sided die**
-   Group outcomes into **mutually exclusive events**
-   Together, these events include **all possible outcomes**
-   The probabilities of all such events must sum to 1
:::

::: {.column width="50%"}
-   Event A: rolling **1–4**
-   Event B: rolling **5–6**

$$
\operatorname{Pr}[1\text{–}4 \text{ or } 5\text{–}6]
=\\
\operatorname{Pr}[1\text{–}4]
+
\operatorname{Pr}[5\text{–}6]
= \\ 0.67 + 0.33 = \\1
$$
:::
:::::

## The [General Multiplication Principle]{.keyword}

-   The probability that **A and B** both occur depends on whether the events are independent
-   In general, the probability of **A and B** equals the probability of **A**, multiplied by the probability of **B given A**

$$
\operatorname{Pr}[A \text{ and } B]
=
\operatorname{Pr}[A] \times \operatorname{Pr}[B \mid A]
$$

-   Read "\|" as "given"

## [Independence]{.keyword}

::::: columns
::: {.column width="50%"}
-   Two events are [independent]{.keyword} if the occurrence of one does **not change** the probability of the other
-   When events are independent:

$$
\operatorname{Pr}[A \mid B] = \operatorname{Pr}[A]
$$

-   In this case, the multiplication rule simplifies
-   This situation is described as independence
:::

::: {.column width="50%"}
![Sample space for rolling two fair dice, illustrating independence between the first and second roll. The probability of rolling a 3 on one roll remains 1/6 regardless of the outcome of the other roll.](images/dice-independence-grid.jpg){#fig-dice-independence-grid fig-alt="A 6 by 6 grid showing all possible outcomes of rolling two dice, labeled as ordered pairs. A highlighted row shows outcomes where the first roll is 3, and a highlighted column shows outcomes where the second roll is 3, illustrating that each has probability one sixth and that the events are independent."}
:::
:::::

## Probabilities for independent variables

-   When two events are **independent**, one does not affect the probability of the other
-   The probability that **both events occur** is the product of their individual probabilities
-   This is a special case of the general multiplication principle

$$
\operatorname{Pr}[A \text{ and } B]
=
\operatorname{Pr}[A] \times \operatorname{Pr}[B]
$$

## Example: Oguchi disease

::::: columns
::: {.column width="50%"}
-   **Oguchi disease** is an **autosomal recessive** condition
-   The disease is expressed only if an individual inherits:
    -   One mutant allele from **mom**
    -   One mutant allele from **dad**
-   Parents who carry one mutant allele typically **do not show symptoms**
:::

::: {.column width="50%"}
![Fundus photographs illustrating retinal changes associated with Oguchi disease and its progression to retinitis pigmentosa, showing characteristic alterations in retinal appearance and vasculature after long-term disease progression (Nishiguchi et al. 2020).](images/oguchi-disease-photos.jpg){#fig-oguchi-disease-photos fig-alt="Side-by-side fundus photographs of the retina showing abnormal retinal coloration and vascular patterns associated with Oguchi disease and its progression to retinitis pigmentosa, with visible changes in retinal structure and blood vessels over long-term disease progression."}
:::
:::::

## Question: Child of Two Oguchi Carriers

:::::: columns
::: {.column width="33%"}
Question

> If both parents have one copy of the disease allele, what is the probability that a given child will have Oguchi disease?
:::

::: {.column width="33%"}
Thinking

> A child has a $\frac{1}{2}$ chance of inheriting mom’s affected chromosome AND a $\frac{1}{2}$ chance of inheriting dad’s affected chromosome.
:::

::: {.column width="33%"}
Answer

> The probability that a given child of heterozygotes has the disease is $\frac{1}{2} \times \frac{1}{2} = \frac{1}{4}$.
:::
::::::

## Visualization: two children of Oguchi carriers

::::: columns
::: {.column width="50%"}
-   If both parents have an affected chromosome but no disease:

    -   What’s the probability that both of their children will have Oguchi disease?

Answer:

$$
\operatorname{Pr}[\text{A affected and B affected}]= \\ \operatorname{Pr}[\text{A affected}] \times \operatorname{Pr}[\text{B affected}]= \\ \frac{1}{4} \times \frac{1}{4}=\frac{1}{16}
$$
:::

::: {.column width="50%"}
![Outcome grid showing the possible numbers of children affected by Oguchi disease when two children are born to heterozygous carrier parents, illustrating the probabilities of zero, one, or two affected children.](images/oguchi-two-children-probability-grid.jpg){#fig-oguchi-two-children-probability-grid fig-alt="A 2×2 grid representing outcomes for two children of Oguchi disease carriers. The horizontal axis indicates whether child two is affected (No, Yes), and the vertical axis indicates whether child one is affected (No, Yes). The cells are labeled with the number of affected children (0, 1, or 2), with darker shading indicating outcomes with more affected children." width="67%"}
:::
:::::

## [Probability trees]{.keyword}

-   [Probability tree]{.keyword}: a diagram that can be used to calculate the probabilities of combinations of events resulting from multiple random trials

Let’s revisit the example of two children of Oguchi carriers

## Probability Tree Step 1: Write down all possible outcomes for event one, two… etc. and connected them

![](images/probability-tree-step-1.jpg){fig-align="center"}

## Probability Tree Step 2: Write down the probability of each outcome, conditional on their path

![](images/probability-tree-step-2.jpg){fig-align="center"}

## Probability Tree Step 3: Sum paths that lead to the same destination

![](images/probability-tree-step-3.jpg){fig-align="center"}

## Probability Tree Step 4

-   Step 1: Write down all possible outcomes for event one, two… etc. and connected them

![](images/probability-tree-step-4.jpg){fig-align="center"}

:::::: columns
::: {.column width="33%"}
$$
\operatorname{Pr}[2 \text{ affected}] = \frac{1}{16}
$$
:::

::: {.column width="33%"}
$$
\operatorname{Pr}[1 \text{ affected}] = \frac{6}{16}
$$
:::

::: {.column width="33%"}
$$
\operatorname{Pr}[0 \text{ affected}] = \frac{9}{16}
$$
:::
::::::

## [Dependent]{.keyword} events

-   Two events are [dependent]{.keyword} if the occurrence of one **changes** the probability of the other
-   Knowing that one event occurred provides information about the other
-   In this case, probabilities cannot be multiplied directly

$$
\operatorname{Pr}[A \text{ and } B]
\neq
\operatorname{Pr}[A] \times \operatorname{Pr}[B]
$$

## Example: surviving the Titanic

::::: columns
::: {.column width="50%"}
-   Of the $2092$ adults on the Titanic:
    -   $319$ (approximately $0.152$) sat in first class (more expensive)
    -   $654$ (approximately $0.312$) survived
-   If survival and sitting in first class are **independent**:
    -   We expect about $0.152 \times 0.312 \times 2092 = 100$ first-class adults to survive
    -   We expect about $0.848 \times 0.312 \times 2092 = 554$ other adults to survive
:::

::: {.column width="50%"}
![Survivors of the RMS Titanic aboard a lifeboat, illustrating unequal survival outcomes during the disaster. Source: Public domain, via Wikimedia Commons.](images/titanic-lifeboat.gif){fig-alt="Survivors of the RMS Titanic aboard a lifeboat, illustrating unequal survival outcomes during the disaster."}
:::
:::::

## Surviving the Titanic depends on class

-   More first-class passengers survived than expected:
    -   $197$ of the $319$ adults in first class survived
    -   This is much higher than the $\approx 100$ survivors expected under independence
-   Fewer other passengers survived than expected:
    -   $457$ of the $1773$ other adults survived
    -   This is much lower than the $\approx 554$ survivors expected under independence
-   Survival was therefore **not independent** of passenger class

## [Conditional probability]{.keyword}

-   The [conditional probability]{.keyword} of an event is the probability that the event occurs **given** that a condition is met
-   Read the symbol `|` as **“given”**
-   $\operatorname{Pr}[X \mid Y]$ means the probability of **X**, given that **Y** is true

## Surviving the Titanic was conditional on class

-   The probability of survival **depends on passenger class**
-   These probabilities are calculated by conditioning on class membership

$$
\operatorname{Pr}[\text{survive} \mid \text{adult in first class}]
=
\frac{197}{319}
=
0.62
$$

$$
\operatorname{Pr}[\text{survive} \mid \text{adult not in first class}]
=
\frac{457}{1773}
=
0.26
$$

## The [Law of Total Probability]{.keyword}

-   The **total probability** of an event can be calculated by summing over all possible conditions
-   Each term is a conditional probability, weighted by how common that condition is

$$
\operatorname{Pr}[X]
=
\sum_i \operatorname{Pr}[X \mid Y_i] \times \operatorname{Pr}[Y_i]
$$

## The total probability of surviving the Titanic

-   Applying this to survival on the Titanic:

$$
\operatorname{Pr}[\text{survive}]
=
\sum_i
\operatorname{Pr}[\text{survive} \mid \text{class}_i]
\times
\operatorname{Pr}[\text{class}_i]
$$

$$
\begin{aligned}
\operatorname{Pr}[\text{survive}]
= 
&\operatorname{Pr}[\text{survive} \mid \text{1st class}] \times \operatorname{Pr}[\text{1st class}]
+\\
&\operatorname{Pr}[\text{survive} \mid \text{not 1st class}] \times \operatorname{Pr}[\text{not 1st class}]
\end{aligned}
$$

$$
\operatorname{Pr}[\text{survive}]
=
0.62 \times 0.152 + 0.26 \times 0.848
=
0.314
$$

## Probability trees for conditional probabilities

## Probability trees for conditional probabilities

::::: columns
::: {.column width="50%"}
-   Apply the probability tree to the **Titanic survival** example
-   Multiply along each path to get **path probabilities**
-   Add the survival paths to get the **overall probability of survival**

$$
\operatorname{Pr}[\text{Survive}]
=
0.094 + 0.220
=
0.314
$$
:::

::: {.column width="50%"}
![Probability tree showing survival on the Titanic by passenger class, with branch probabilities for first class versus not first class and conditional survival outcomes.](images/titanic-probability-tree.jpg){#fig-titanic-probability-tree fig-alt="A probability tree diagram for adult Titanic passengers, split first by class and then by survival outcome. Each complete path shows the combined probability, and the two survival paths are summed to obtain the total probability of surviving."}
:::
:::::

## Summary: the addition principle

-   Use the **addition principle** when calculating the probability of **A or B**
-   Add probabilities of events that can occur **instead of** one another
-   Subtract any overlap to avoid **double counting**

$$
\operatorname{Pr}[A \text{ or } B]
=
\operatorname{Pr}[A]
+
\operatorname{Pr}[B]
-
\operatorname{Pr}[A \text{ and } B]
$$

-   If events are **mutually exclusive**, the overlap term is $0$

## Summary: the multiplication principle

-   Use the **multiplication principle** when calculating the probability of **A and B**
-   Multiply probabilities of events that occur **together**
-   Conditional probability is required when events are **dependent**

$$
\operatorname{Pr}[A \text{ and } B]
=
\operatorname{Pr}[A] \times \operatorname{Pr}[B \mid A]
$$

-   If events are **independent**, this simplifies to\
    $\operatorname{Pr}[A \text{ and } B] = \operatorname{Pr}[A] \times \operatorname{Pr}[B]$

## [Bayes’ theorem]{.keyword}

-   Bayes’ theorem allows us to **reverse a conditional probability**
-   It tells us how to find the probability of **A given B** using:
    -   The probability of **B given A**
    -   How common **A** is
    -   The overall probability of **B**

$$
\operatorname{Pr}[A \mid B]
=
\frac{\operatorname{Pr}[B \mid A] \times \operatorname{Pr}[A]}
     {\operatorname{Pr}[B]}
$$

-   Bayes’ theorem is **not a new rule**, but a rearrangement of ideas you already know

## Applying Bayes’ theorem: the Titanic

-   Find the probability that an **adult survivor** was in **first class**
-   Of the $2092$ adults on the Titanic:
    -   $319$ were in first class
    -   $197$ of the $319$ first-class adults survived
    -   $457$ of the other adults survived

$$
\operatorname{Pr}[\text{1st class} \mid \text{survive}]
=
\frac{
\operatorname{Pr}[\text{survive} \mid \text{1st class}]
\times
\operatorname{Pr}[\text{1st class}]
}{
\operatorname{Pr}[\text{survive}]
}
$$

$$
\operatorname{Pr}[\text{1st class} \mid \text{survive}]
=
\frac{
\left(\frac{197}{319}\right)
\times
\left(\frac{319}{2092}\right)
}{
\left(\frac{197 + 457}{2092}\right)
}
=
0.301
$$

## When Bayes’ theorem applies

-   Bayes’ theorem is used to **reverse a conditional probability**
-   It applies when:
    -   You know $\operatorname{Pr}[B \mid A]$, and
    -   You want $\operatorname{Pr}[A \mid B]$
-   The condition you **observe** is not the condition you **care about**

> Use Bayes’ theorem when the probability you want is reversed from the probability you know.

## Bayes’ theorem: examples and takeaway

::::: columns
::: {.column width="50%"}
**Examples where Bayes applies**

-   Medical testing:\
    Known $\operatorname{Pr}[\text{positive} \mid \text{disease}]$ →\
    Want $\operatorname{Pr}[\text{disease} \mid \text{positive}]$
-   Titanic:\
    Known $\operatorname{Pr}[\text{survive} \mid \text{class}]$ →\
    Want $\operatorname{Pr}[\text{class} \mid \text{survive}]$

**Example where Bayes is not needed**

-   If you already have $\operatorname{Pr}[A \mid B]$ and that is what you want
:::

::: {.column width="50%"}
**What to know:**

-   Bayes’ theorem helps compute the probability of a **cause given an observed outcome**
-   It combines conditional probability, prior probability, and total probability
-   You should be able to **recognize when Bayes’ theorem applies and interpret a worked example**
:::
:::::
